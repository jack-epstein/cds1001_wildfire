{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still to do: Print/graph evaluation in digestible format, interate and best phase 1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd  # provides interface for interacting with tabular data\n",
    "import geopandas as gpd  # combines the capabilities of pandas and shapely for geospatial operations\n",
    "import rtree  # supports geospatial join\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "from shapely.ops import nearest_points\n",
    "from datetime import datetime as dt, date\n",
    "sys.path.append('/Users/jackepstein/Documents/GitHub/wildfires-1001/code/functions/')\n",
    "from modeling_functions import *\n",
    "data_dir = '/Users/jackepstein/Documents/GitHub/wildfires-1001/data'\n",
    "code_dir = '/Users/jackepstein/Documents/GitHub/wildfires-1001/code'\n",
    "model_dir = '/Users/jackepstein/Documents/GitHub/wildfires-1001/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, multilabel_confusion_matrix\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import scale, label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull in main data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in the target data frame and weather dictionary \n",
    "#make sure to change the pkl file name if needed\n",
    "target_dict = {}\n",
    "target_df = gpd.GeoDataFrame()\n",
    "for i in np.arange(1, 3):\n",
    "    target_dict[i] = pd.read_pickle(os.path.join(data_dir, f'clean_data/target_df_final_1123_newtargets_{i}.pkl')) \n",
    "    target_df = target_df.append(target_dict[i])\n",
    "\n",
    "\n",
    "weather_dict_path = os.path.join(data_dir, 'clean_data/ERA_weather-data/ERA_rename_dictionary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the naming dictionary\n",
    "with open(weather_dict_path, 'rb') as handle:\n",
    "    rename_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the columns based on this dictionary\n",
    "target_df.rename(columns = rename_dict, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists of columns to drop and what our targets are\n",
    "non_mod_cols = ['GRID_ID','month_id','MONTH','COUNTYFP','COUNTY_AREA', 'NAME','GRID_AREA','COUNTY_ARE','month_id_old_x','month_id_old_y',\n",
    "                'geometry','Fire_area','total_fire_days','hist_p_time_1y','total_fire_days','hist_p_time_1y', \n",
    "                'hist_p_time_1m', 'month_id_old', 'YEAR','Index','index']\n",
    "bad_features = ['hist_p_time_1m', 'total_fire_days', 'hist_p_time_1y','month_id_old']\n",
    "Y_cols = ['Y_bin', 'Y_fire_count', 'Y_fire_area_prop', 'Y_fire_class_size','Y_bin_new_fire_month',\n",
    "          'Y_max_new_fire_size_month','Y_count_new_fires_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert floats from 64 to 32 for model\n",
    "for col in target_df.columns:\n",
    "    if target_df[col].dtypes == 'float64':\n",
    "        target_df[col] = target_df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull in Models and Feature Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in models\n",
    "\n",
    "#list of models\n",
    "model_list = ['LR_15PCA_1990_2015.pkl', 'LR_30entropy_1990_2015.pkl', 'linSVC_25PCA_1990_2015.pkl', \n",
    "              'LR_15PCA_1990_2005.pkl', 'LR_20gini_1990_2005.pkl', 'linSVC_15PCA_1990_2005.pkl', \n",
    "              'linSVC_30gini_1990_2005.pkl', 'linSVC_35entropy_1990_2015.pkl']\n",
    "\n",
    "\n",
    "#get all paths for loading\n",
    "model_path_list = []\n",
    "for m in model_list:\n",
    "    mod_path = os.path.join(model_dir, m)\n",
    "    model_path_list.append(mod_path)\n",
    "\n",
    "#load the models into a dictionary with the file as the key and the model as the value\n",
    "models = {}\n",
    "for m in range(len(model_list)):\n",
    "    with open(model_path_list[m], 'rb') as handle:\n",
    "        models[model_list[m]] = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in feature lists\n",
    "\n",
    "#w ill need 30e\n",
    "feat_list = ['RF_entropy_top30_features.pkl', 'RF_gini_top20_features_1990_2005.pkl', \n",
    "             'RF_gini_top30_features_1990_2005.pkl', 'RF_entropy_top35_features.pkl']\n",
    "\n",
    "#get paths for loading\n",
    "feat_path_list = []\n",
    "for f in feat_list:\n",
    "    feat_path = os.path.join(model_dir,'feature_lists',f)\n",
    "    feat_path_list.append(feat_path)\n",
    "\n",
    "features = {}\n",
    "for f in range(len(feat_list)):\n",
    "    with open(feat_path_list[f], 'rb') as handle:\n",
    "        features[feat_list[f]] = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the split where using the 1990-2015 data all as training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate training data set\n",
    "#pre 2016\n",
    "train_data = target_df[target_df['YEAR']<2016]\n",
    "X_train = train_data.drop('YEAR', axis = 1)\n",
    "#drop columns not used for modeling - dont drop Ys here\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_train.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "#set up target variable\n",
    "Y_train_cl = train_data[['Y_bin_new_fire_month']]\n",
    "Y_train_cl_size = train_data[['Y_max_new_fire_size_month']]\n",
    "Y_train_cl_arr = Y_train_cl.to_numpy().ravel()\n",
    "Y_train_size_arr = Y_train_cl_size.to_numpy().ravel()\n",
    "\n",
    "#generate testing data set - same logic as above\n",
    "test_data = target_df[target_df['YEAR']>=2016]\n",
    "X_test = test_data.drop('YEAR', axis = 1)\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_test.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "Y_test_cl = test_data[['Y_bin_new_fire_month']]\n",
    "Y_test_cl_size = test_data[['Y_max_new_fire_size_month']]\n",
    "Y_test_cl_arr = Y_test_cl.to_numpy().ravel()\n",
    "Y_test_size_arr = Y_test_cl_size.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the split where using the 1990-2005 on initial test, 2006-2015 on 2nd stage and 2016-2019 on final testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate phase1 data set\n",
    "#pre 2006\n",
    "phase1_data = target_df[target_df['YEAR']<2006]\n",
    "X_phase1 = phase1_data.drop('YEAR', axis = 1)\n",
    "#drop columns not used for modeling - dont drop Ys here\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_phase1.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "#set up target variable\n",
    "Y_ph1_cl = phase1_data[['Y_bin_new_fire_month']]\n",
    "Y_ph1_cl_size = phase1_data[['Y_max_new_fire_size_month']]\n",
    "Y_ph1_cl_arr = Y_ph1_cl.to_numpy().ravel()\n",
    "Y_ph1_cl_size_arr = Y_ph1_cl_size.to_numpy().ravel()\n",
    "\n",
    "#generate phase2 data set - same logic as above\n",
    "phase2_data = target_df[(target_df['YEAR']>=2006)&(target_df['YEAR']<2016)]\n",
    "X_phase2 = phase2_data.drop('YEAR', axis = 1)\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_phase2.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "Y_ph2_cl = phase2_data[['Y_bin_new_fire_month']]\n",
    "Y_ph2_cl_size = phase2_data[['Y_max_new_fire_size_month']]\n",
    "Y_ph2_cl_arr = Y_ph2_cl.to_numpy().ravel()\n",
    "Y_ph2_cl_size_arr = Y_ph2_cl_size.to_numpy().ravel()\n",
    "\n",
    "#generate phase3 (test) data set\n",
    "phase3_data = target_df[target_df['YEAR']>=2016]\n",
    "X_phase3 = phase3_data.drop('YEAR', axis = 1)\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_phase3.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "Y_ph3_cl = phase3_data[['Y_bin_new_fire_month']]\n",
    "Y_ph3_cl_size = phase3_data[['Y_max_new_fire_size_month']]\n",
    "Y_ph3_cl_arr = Y_ph3_cl.to_numpy().ravel()\n",
    "Y_ph3_cl_size_arr = Y_ph3_cl_size.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale all data sets\n",
    "X_train_scaled = pd.DataFrame(scale(X_train), columns = X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scale(X_test), columns = X_test.columns, index=X_test.index)\n",
    "X_phase1_scaled = pd.DataFrame(scale(X_phase1), columns = X_test.columns, index=X_phase1.index)\n",
    "X_phase2_scaled = pd.DataFrame(scale(X_phase2), columns = X_test.columns, index=X_phase2.index)\n",
    "X_phase3_scaled = pd.DataFrame(scale(X_phase3), columns = X_test.columns, index=X_phase3.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regularization hyperparam options\n",
    "cs = [10**i for i in range(-4, 2)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1: Taking LR model, 30entropy features, 1990-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1A. Take all instances in the training set that are predicted binary positive and test size classification on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. slim down features\n",
    "X_tr_sc_30featentr = X_train_scaled[features['RF_entropy_top30_features.pkl']]\n",
    "X_test_sc_30featentr = X_test_scaled[features['RF_entropy_top30_features.pkl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. run model on train set to get predictions\n",
    "# -- run this on the test data as well because we will filter for the positive preds\n",
    "y_preds_test1 = models['LR_30entropy_1990_2015.pkl'].predict(X_tr_sc_30featentr)\n",
    "y_preds_1 = models['LR_30entropy_1990_2015.pkl'].predict(X_test_sc_30featentr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. store predictions and filter\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds = pd.DataFrame(y_preds_test1, columns=['preds'], index=X_tr_sc_30featentr.index)\n",
    "X_tr_sc_30featentr_preds = X_tr_sc_30featentr.merge(preds, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_tr_sc_30featentr_ysize = X_tr_sc_30featentr_preds.merge(Y_train_cl_size, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for the positive predicted instances\n",
    "X_tr_sc_30featentr_cut = X_tr_sc_30featentr_ysize.loc[X_tr_sc_30featentr_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_train_cl_size_cut1 = X_tr_sc_30featentr_cut['Y_max_new_fire_size_month']\n",
    "X_tr_sc_30featentr_ready = X_tr_sc_30featentr_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.5 need to score the test set and select only the positives to score on\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "test_set_preds_1 = pd.DataFrame(y_preds_1, columns=['preds'], index=X_test_sc_30featentr.index)\n",
    "X_test_sc_30featentr_preds = X_test_sc_30featentr.merge(test_set_preds_1, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_test_sc_30featentr_ysize = X_test_sc_30featentr_preds.merge(Y_test_cl_size, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for only positive predictions \n",
    "X_test_sc_30featentr_cut = X_test_sc_30featentr_ysize.loc[X_test_sc_30featentr_preds['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_test_cl_size_cut1 = X_test_sc_30featentr_cut['Y_max_new_fire_size_month']\n",
    "X_test_sc_30featentr_ready = X_test_sc_30featentr_cut.drop(columns=['preds','Y_max_new_fire_size_month'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t1 = {}\n",
    "aucs_lr_t1 = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_tr_sc_30featentr_ready, \n",
    "                                                                               Y_train_cl_size_cut1.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_cl_size_cut1, lr.predict(X_test_sc_30featentr_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_cl_size_cut1, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_test_sc_30featentr_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t1[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t1[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.6831427606503595,\n",
       " 0.001: 0.67118673710619,\n",
       " 0.01: 0.6630210594876248,\n",
       " 0.1: 0.6603524388478796,\n",
       " 1: 0.6601172383847156,\n",
       " 10: 0.6601715154146764}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t1 = {}\n",
    "aucs_svm_t1 = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_tr_sc_30featentr_ready, \n",
    "                                                                               Y_train_cl_size_cut1.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_cl_size_cut1, svm.predict(X_test_sc_30featentr_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_cl_size_cut1, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_test_sc_30featentr_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t1[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t1[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.5998998890336277,\n",
       " 0.001: 0.7039972017175664,\n",
       " 0.01: 0.7016150431803928,\n",
       " 0.1: 0.7008431031987262,\n",
       " 1: 0.7006923336710571,\n",
       " 10: 0.7006802721088435}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B. Same test data set and same features, only predicting on the actual positive instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only the postitve instances (not positive preds)\n",
    "X_tr_sc_30featentr_cut2 = X_tr_sc_30featentr_ysize.loc[X_tr_sc_30featentr_ysize['Y_max_new_fire_size_month']>0]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_train_cl_size_cut2 = X_tr_sc_30featentr_cut2['Y_max_new_fire_size_month']\n",
    "X_tr_sc_30featentr_ready2 = X_tr_sc_30featentr_cut2.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t1b = {}\n",
    "aucs_lr_t1b = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_tr_sc_30featentr_ready2, \n",
    "                                                                               Y_train_cl_size_cut2.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_cl_size_cut1, lr.predict(X_test_sc_30featentr_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_cl_size_cut1, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_test_sc_30featentr_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t1b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t1b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.5602296893188707,\n",
       " 0.001: 0.521843931372912,\n",
       " 0.01: 0.49920406727791006,\n",
       " 0.1: 0.49723377387284007,\n",
       " 1: 0.4971227134930135,\n",
       " 10: 0.4970281065027909}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t1b = {}\n",
    "aucs_svm_t1b = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_tr_sc_30featentr_ready2, \n",
    "                                                                               Y_train_cl_size_cut2.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_cl_size_cut1, svm.predict(X_test_sc_30featentr_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_cl_size_cut1, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_test_sc_30featentr_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t1b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t1b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.43072917309377196,\n",
       " 0.001: 0.47963276034404034,\n",
       " 0.01: 0.5036958426397818,\n",
       " 0.1: 0.5074924622908877,\n",
       " 1: 0.5078503235147731,\n",
       " 10: 0.5079367038101936}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: Taking LR model, 20gini features, train on 2006-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2A. Use predictive positives in 2006-2015 as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the 'LR_20gini_1990_2005.pkl' model\n",
    "#use the 'RF_gini_top20_features_1990_2005.pkl' feature list\n",
    "#key -- use 1990-2005 to get predictions of positive instances in 2006-2015 using imported model\n",
    "#then use these positive predictions to train LR/SVM models for class size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. slim down features\n",
    "X_ph1_sc_20gi = X_phase1_scaled[features['RF_gini_top20_features_1990_2005.pkl']]\n",
    "X_ph2_sc_20gi = X_phase2_scaled[features['RF_gini_top20_features_1990_2005.pkl']]\n",
    "X_ph3_sc_20gi = X_phase3_scaled[features['RF_gini_top20_features_1990_2005.pkl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. run model on train set to get predictions\n",
    "y_preds_test2 = models['LR_20gini_1990_2005.pkl'].predict(X_ph2_sc_20gi)\n",
    "y_preds_2 = models['LR_20gini_1990_2005.pkl'].predict(X_ph3_sc_20gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. store predictions and filter\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds2 = pd.DataFrame(y_preds_test2, columns=['preds'], index=X_ph2_sc_20gi.index)\n",
    "X_ph2_sc_20gi_preds = X_ph2_sc_20gi.merge(preds2, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_ph2_sc_20gi_ysize = X_ph2_sc_20gi_preds.merge(Y_ph2_cl_size, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for the positive predicted instances\n",
    "X_ph2_sc_20gi_cut = X_ph2_sc_20gi_ysize.loc[X_ph2_sc_20gi_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph2_cl_size_cut1 = X_ph2_sc_20gi_cut['Y_max_new_fire_size_month']\n",
    "X_ph2_sc_20gi_ready = X_ph2_sc_20gi_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.5 need to score the test set and select only the positives to score on\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "test_set_preds_2 = pd.DataFrame(y_preds_2, columns=['preds'], index=X_ph3_sc_20gi.index)\n",
    "X_ph3_sc_20gi_preds = X_ph3_sc_20gi.merge(test_set_preds_2, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_ph3_sc_20gi_ysize = X_ph3_sc_20gi_preds.merge(Y_ph3_cl_size, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for only positive predictions \n",
    "X_ph3_sc_20gi_cut = X_ph3_sc_20gi_ysize.loc[X_ph3_sc_20gi_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph3_cl_size_cut2 = X_ph3_sc_20gi_cut['Y_max_new_fire_size_month']\n",
    "X_ph3_sc_20gi_ready = X_ph3_sc_20gi_cut.drop(columns=['preds','Y_max_new_fire_size_month'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t2 = {}\n",
    "aucs_lr_t2 = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_ph2_sc_20gi_ready, \n",
    "                                                                               Y_ph2_cl_size_cut1.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_cut2, lr.predict(X_ph3_sc_20gi_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_cut2, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_ph3_sc_20gi_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t2[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t2[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.6719190414105668,\n",
       " 0.001: 0.6678524864965543,\n",
       " 0.01: 0.6767306140187497,\n",
       " 0.1: 0.6788104550816416,\n",
       " 1: 0.6789718755820451,\n",
       " 10: 0.6790432731110697}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t2 = {}\n",
    "aucs_svm_t2 = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_ph2_sc_20gi_ready, \n",
    "                                                                               Y_ph2_cl_size_cut1.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_cut2, svm.predict(X_ph3_sc_20gi_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_cut2, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_ph3_sc_20gi_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t2[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t2[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.4480939963990811,\n",
       " 0.001: 0.6295896194201278,\n",
       " 0.01: 0.6951263425839698,\n",
       " 0.1: 0.6967995281554604,\n",
       " 1: 0.6968771341652698,\n",
       " 10: 0.6969174892903707}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B. Same test data set and same features, only predicting on the actual positive instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only the postitve instances in the phase 2 set(not positive preds)\n",
    "X_ph2_sc_20gi_cut2 = X_ph2_sc_20gi_ysize.loc[X_ph2_sc_20gi_ysize['Y_max_new_fire_size_month']>0]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph2_cl_size_cut2 = X_ph2_sc_20gi_cut2['Y_max_new_fire_size_month']\n",
    "X_ph2_sc_20gi_ready2 = X_ph2_sc_20gi_cut2.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t2b = {}\n",
    "aucs_lr_t2b = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_ph2_sc_20gi_ready2, \n",
    "                                                                               Y_ph2_cl_size_cut2.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_cut2, lr.predict(X_ph3_sc_20gi_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_cut2, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_ph3_sc_20gi_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t2b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t2b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.5631791917003185,\n",
       " 0.001: 0.5434928920844413,\n",
       " 0.01: 0.5104525755934206,\n",
       " 0.1: 0.4913490265602941,\n",
       " 1: 0.48590728520305987,\n",
       " 10: 0.4851932105453232}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t2b = {}\n",
    "aucs_svm_t2b = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_ph2_sc_20gi_ready2, \n",
    "                                                                               Y_ph2_cl_size_cut2.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_cut2, svm.predict(X_ph3_sc_20gi_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_cut2, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_ph3_sc_20gi_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t2b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t2b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.41089251124462395,\n",
       " 0.001: 0.4768705472930825,\n",
       " 0.01: 0.497607439508848,\n",
       " 0.1: 0.49863340884467644,\n",
       " 1: 0.49846925375094386,\n",
       " 10: 0.49848156538297383}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3: Taking SVM model, 30entropy features, 1990-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 'linSVC_35entropy_1990_2015.pkl' model <br>\n",
    "use 'RF_entropy_top35_features.pkl' features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A. Take all instances in the training set that are predicted binary positive and test size classification on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. slim down features\n",
    "X_tr_sc_35featentr = X_train_scaled[features['RF_entropy_top35_features.pkl']]\n",
    "X_test_sc_35featentr = X_test_scaled[features['RF_entropy_top35_features.pkl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. run model on train set to get predictions\n",
    "y_preds_test3 = models['linSVC_35entropy_1990_2015.pkl'].predict(X_tr_sc_35featentr)\n",
    "y_preds_3 = models['linSVC_35entropy_1990_2015.pkl'].predict(X_test_sc_35featentr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. store predictions and filter\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds3 = pd.DataFrame(y_preds_test3, columns=['preds'], index=X_tr_sc_35featentr.index)\n",
    "X_tr_sc_35featentr_preds = X_tr_sc_35featentr.merge(preds3, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_tr_sc_35featentr_ysize = X_tr_sc_35featentr_preds.merge(Y_train_cl_size, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for the positive predicted instances\n",
    "X_tr_sc_35featentr_cut = X_tr_sc_35featentr_ysize.loc[X_tr_sc_35featentr_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_train_cl_size_cut3 = X_tr_sc_35featentr_cut['Y_max_new_fire_size_month']\n",
    "X_tr_sc_35featentr_ready = X_tr_sc_35featentr_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.5 need to score the test set and select only the positives to score on\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "test_set_preds_3 = pd.DataFrame(y_preds_3, columns=['preds'], index=X_test_sc_35featentr.index)\n",
    "X_test_sc_35featentr_preds = X_test_sc_35featentr.merge(test_set_preds_3, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_test_sc_35featentr_ysize = X_test_sc_35featentr_preds.merge(Y_test_cl_size, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for only positive predictions \n",
    "X_test_sc_35featentr_cut = X_test_sc_35featentr_ysize.loc[X_test_sc_35featentr_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_test_cl_size_cut3 = X_test_sc_35featentr_cut['Y_max_new_fire_size_month']\n",
    "X_test_sc_35featentr_ready = X_test_sc_35featentr_cut.drop(columns=['preds','Y_max_new_fire_size_month'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t3 = {}\n",
    "aucs_lr_t3 = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_tr_sc_35featentr_ready, \n",
    "                                                                               Y_train_cl_size_cut3.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_cl_size_cut3, lr.predict(X_test_sc_35featentr_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_cl_size_cut3, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_test_sc_35featentr_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t3[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t3[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.6670552539124881,\n",
       " 0.001: 0.6675582880868731,\n",
       " 0.01: 0.6649872245289046,\n",
       " 0.1: 0.6648461620355585,\n",
       " 1: 0.6650644096667732,\n",
       " 10: 0.665061748110295}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t3 = {}\n",
    "aucs_svm_t3 = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_tr_sc_35featentr_ready, \n",
    "                                                                               Y_train_cl_size_cut3.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_cl_size_cut3, svm.predict(X_test_sc_35featentr_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_cl_size_cut3, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_test_sc_35featentr_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t3[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t3[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.6120089428297668,\n",
       " 0.001: 0.70482274033855,\n",
       " 0.01: 0.7020547216011924,\n",
       " 0.1: 0.7014585329500693,\n",
       " 1: 0.7014478867241563,\n",
       " 10: 0.7014319173852869}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Same test data set and same features, only predicting on the actual positive instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only the postitve instances (not positive preds)\n",
    "X_tr_sc_35featentr_cut3 = X_tr_sc_35featentr_ysize.loc[X_tr_sc_35featentr_ysize['Y_max_new_fire_size_month']>0]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_train_cl_size_cut3b = X_tr_sc_35featentr_cut3['Y_max_new_fire_size_month']\n",
    "X_tr_sc_35featentr_ready2 = X_tr_sc_35featentr_cut3.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t3b = {}\n",
    "aucs_lr_t3b = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_tr_sc_35featentr_ready2, \n",
    "                                                                               Y_train_cl_size_cut3b.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_cl_size_cut3, lr.predict(X_test_sc_35featentr_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_cl_size_cut3, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_test_sc_35featentr_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t3b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t3b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.5461430525212696,\n",
       " 0.001: 0.5238734658354204,\n",
       " 0.01: 0.5113301717553862,\n",
       " 0.1: 0.5131560718282091,\n",
       " 1: 0.5132271185236497,\n",
       " 10: 0.5132093568497895}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t3b = {}\n",
    "aucs_svm_t3b = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_tr_sc_35featentr_ready2, \n",
    "                                                                               Y_train_cl_size_cut3b.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_cl_size_cut3, svm.predict(X_test_sc_35featentr_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_cl_size_cut3, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_test_sc_35featentr_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t3b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t3b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.44139890943322496,\n",
       " 0.001: 0.488581019875313,\n",
       " 0.01: 0.5175538622759809,\n",
       " 0.1: 0.522733166373599,\n",
       " 1: 0.5232873305980356,\n",
       " 10: 0.5233761389673363}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 4: Taking SVM model, 30entropy features, 2005-2015 as testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 'linSVC_30gini_1990_2005.pkl' model <br>\n",
    "use 'RF_gini_top30_features_1990_2005.pkl' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. slim down features\n",
    "X_ph1_sc_30gi = X_phase1_scaled[features['RF_gini_top30_features_1990_2005.pkl']]\n",
    "X_ph2_sc_30gi = X_phase2_scaled[features['RF_gini_top30_features_1990_2005.pkl']]\n",
    "X_ph3_sc_30gi = X_phase3_scaled[features['RF_gini_top30_features_1990_2005.pkl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. run model on train set to get predictions\n",
    "y_preds_test4 = models['linSVC_30gini_1990_2005.pkl'].predict(X_ph2_sc_30gi)\n",
    "y_preds_4 = models['linSVC_30gini_1990_2005.pkl'].predict(X_ph3_sc_30gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. store predictions and filter\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds4 = pd.DataFrame(y_preds_test4, columns=['preds'], index=X_ph2_sc_30gi.index)\n",
    "X_ph2_sc_30gi_preds = X_ph2_sc_30gi.merge(preds4, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_ph2_sc_30gi_ysize = X_ph2_sc_30gi_preds.merge(Y_ph2_cl_size, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for the positive predicted instances\n",
    "X_ph2_sc_30gi_cut = X_ph2_sc_30gi_ysize.loc[X_ph2_sc_30gi_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph2_cl_size_cut4 = X_ph2_sc_30gi_cut['Y_max_new_fire_size_month']\n",
    "X_ph2_sc_30gi_ready = X_ph2_sc_30gi_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.5 need to score the test set and select only the positives to score on\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "test_set_preds_4 = pd.DataFrame(y_preds_4, columns=['preds'], index=X_ph3_sc_20gi.index)\n",
    "X_ph3_sc_30gi_preds = X_ph3_sc_30gi.merge(test_set_preds_4, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_ph3_sc_30gi_ysize  = X_ph3_sc_30gi_preds.merge(Y_ph3_cl_size, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for only positive predictions \n",
    "X_ph3_sc_30gi_cut = X_ph3_sc_30gi_ysize.loc[X_ph3_sc_30gi_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph3_cl_size_cut4 = X_ph3_sc_30gi_cut['Y_max_new_fire_size_month']\n",
    "X_ph3_sc_30gi_ready = X_ph3_sc_30gi_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. run LR on all Cs and store all confusion matrices and AUCs\n",
    "aucs_lr_t4 = {}\n",
    "conf_mats_lr_t4 = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_ph2_sc_30gi_ready, \n",
    "                                                                               Y_ph2_cl_size_cut4.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_cut4, lr.predict(X_ph3_sc_30gi_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_cut4, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_ph3_sc_30gi_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t4[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t4[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.6169911309019982,\n",
       " 0.001: 0.6427230812098144,\n",
       " 0.01: 0.6700190840697962,\n",
       " 0.1: 0.6790774455735998,\n",
       " 1: 0.6805179511174433,\n",
       " 10: 0.6807045859598025}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. run SVM on all Cs and store all confusion matrices and AUCs\n",
    "aucs_svm_t4 = {}\n",
    "conf_mats_svm_t4 = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_ph2_sc_30gi_ready, \n",
    "                                                                               Y_ph2_cl_size_cut4.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_cut4, svm.predict(X_ph3_sc_30gi_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_cut4, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_ph3_sc_30gi_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t4[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t4[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.47086130665418935,\n",
       " 0.001: 0.6386828311716989,\n",
       " 0.01: 0.6966579219918931,\n",
       " 0.1: 0.6993470409177177,\n",
       " 1: 0.6994863598845493,\n",
       " 10: 0.6994811025650461}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4B. Same test data set and same features, only predicting on the actual positive instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only the postitve instances in the phase 2 set(not positive preds)\n",
    "X_ph2_sc_30gi_cut4 = X_ph2_sc_30gi_ysize.loc[X_ph2_sc_30gi_ysize['Y_max_new_fire_size_month']>0]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph2_cl_size_cut4 = X_ph2_sc_30gi_cut4['Y_max_new_fire_size_month']\n",
    "X_ph2_sc_30gi_ready4 = X_ph2_sc_30gi_cut4.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t4b = {}\n",
    "aucs_lr_t4b = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_ph2_sc_30gi_ready4, \n",
    "                                                                               Y_ph2_cl_size_cut4.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_cut4, lr.predict(X_ph3_sc_30gi_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_cut4, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_ph3_sc_30gi_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t4b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t4b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.4987205244792217,\n",
       " 0.001: 0.5078848119558704,\n",
       " 0.01: 0.499784991716894,\n",
       " 0.1: 0.49021536075570127,\n",
       " 1: 0.48720876951817,\n",
       " 10: 0.48675760459624257}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t4b = {}\n",
    "aucs_svm_t4b = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_ph2_sc_30gi_ready4, \n",
    "                                                                               Y_ph2_cl_size_cut4.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_cl_size_cut4, svm.predict(X_ph3_sc_30gi_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_cl_size_cut4, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_ph3_sc_30gi_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t4b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t4b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.4293644919107539,\n",
       " 0.001: 0.4675126008952804,\n",
       " 0.01: 0.4894822177575694,\n",
       " 0.1: 0.4942934686828098,\n",
       " 1: 0.4949561171618907,\n",
       " 10: 0.49503013640689436}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy base df and re-drop non modeling columns\n",
    "X_features = target_df.copy()\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_features.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "#scale features    \n",
    "X_features_scaled = scale(X_features)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f95b317cd90>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fit the scaled dataset\n",
    "pca_x = PCA().fit(X_features_scaled)\n",
    "\n",
    "#plot explained variance ratio\n",
    "plt.plot(pca_x.explained_variance_ratio_[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(pca_x.components_[0:20,].transpose(), columns = [\"PC{}\".format(i+1) for i in range(20)], \\\n",
    "                      index = X_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write for loop to make list \"nth_principle_component\"\n",
    "princ_comp_list = []\n",
    "for name in range(1,173):\n",
    "    pr = str(name)+'th_pc'\n",
    "    princ_comp_list.append(pr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do split on 2 phases -- still need to do 3 phase split as well\n",
    "X_features_pca = pd.DataFrame(pca_x.transform(X_features_scaled), columns = princ_comp_list)\n",
    "X_features_pca['YEAR'] = target_df['YEAR']\n",
    "\n",
    "#split test and train on year\n",
    "X_train_pca = X_features_pca[X_features_pca['YEAR']< 2016]\n",
    "X_test_pca = X_features_pca[X_features_pca['YEAR']>=2016]\n",
    "\n",
    "Y_train_pca = target_df[target_df['YEAR']<2016]['Y_bin_new_fire_month']\n",
    "Y_train_size_pca = target_df[target_df['YEAR']<2016]['Y_max_new_fire_size_month']\n",
    "Y_test_pca = target_df[target_df['YEAR']>=2016]['Y_bin_new_fire_month']\n",
    "Y_test_size_pca = target_df[target_df['YEAR']>=2016]['Y_max_new_fire_size_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do split on 3 phases -- still need to do 3 phase split as well\n",
    "X_features_pca = pd.DataFrame(pca_x.transform(X_features_scaled), columns = princ_comp_list)\n",
    "X_features_pca['YEAR'] = target_df['YEAR']\n",
    "\n",
    "#split phases on year\n",
    "X_ph1_pca = X_features_pca[X_features_pca['YEAR']< 2006]\n",
    "X_ph2_pca = X_features_pca[(X_features_pca['YEAR']>=2006)&(X_features_pca['YEAR']<2016)]\n",
    "X_ph3_pca = X_features_pca[X_features_pca['YEAR']>=2016]\n",
    "\n",
    "\n",
    "Y_ph1_pca = target_df[target_df['YEAR']<2006]['Y_bin_new_fire_month']\n",
    "Y_ph1_size_pca = target_df[target_df['YEAR']<2006]['Y_max_new_fire_size_month']\n",
    "\n",
    "Y_ph2_pca = target_df[(target_df['YEAR']>=2006)&(target_df['YEAR']<2016)]['Y_bin_new_fire_month']\n",
    "Y_ph2_size_pca = target_df[(target_df['YEAR']>=2006)&(target_df['YEAR']<2016)]['Y_max_new_fire_size_month']\n",
    "\n",
    "Y_ph3_pca = target_df[target_df['YEAR']>=2016]['Y_bin_new_fire_month']\n",
    "Y_ph3_size_pca = target_df[target_df['YEAR']>=2016]['Y_max_new_fire_size_month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 5: Taking LR model-PCA, training on 1990-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 'LR_15PCA_1990_2015.pkl' model <br>\n",
    "use top 15 principle directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. slim down features\n",
    "X_tr_lrpca15 = X_train_pca[X_train_pca.columns[0:15]]\n",
    "X_test_lrpca15 = X_test_pca[X_test_pca.columns[0:15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. run model on train and test sets to get predictions\n",
    "y_preds_test5 = models['LR_15PCA_1990_2015.pkl'].predict(X_tr_lrpca15)\n",
    "y_preds_5 = models['LR_15PCA_1990_2015.pkl'].predict(X_test_lrpca15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. store predictions and filter\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds5 = pd.DataFrame(y_preds_test5, columns=['preds'], index=X_tr_lrpca15.index)\n",
    "X_tr_lrpca15_preds = X_tr_lrpca15.merge(preds5, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_tr_lrpca15_ysize = X_tr_lrpca15_preds.merge(Y_train_size_pca, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for the positive predicted instances\n",
    "X_tr_lrpca15_cut = X_tr_lrpca15_ysize.loc[X_tr_lrpca15_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_train_size_pca_cut5 = X_tr_lrpca15_cut['Y_max_new_fire_size_month']\n",
    "X_tr_lrpca15_ready = X_tr_lrpca15_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.5 need to score the test set and select only the positives to score on\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "test_set_preds_5 = pd.DataFrame(y_preds_5, columns=['preds'], index=X_test_lrpca15.index)\n",
    "X_test_lrpca15_preds = X_test_lrpca15.merge(test_set_preds_5, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_test_lrpca15_ysize = X_test_lrpca15_preds.merge(Y_test_size_pca, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for only positive predictions \n",
    "X_test_lrpca15_cut = X_test_lrpca15_ysize.loc[X_test_lrpca15_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_test_size_pca_cut5 = X_test_lrpca15_cut['Y_max_new_fire_size_month']\n",
    "X_test_lrpca15_ready = X_test_lrpca15_cut.drop(columns=['preds','Y_max_new_fire_size_month'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t5 = {}\n",
    "aucs_lr_t5 = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_tr_lrpca15_ready, \n",
    "                                                                               Y_train_size_pca_cut5.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_pca_cut5, lr.predict(X_test_lrpca15_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_pca_cut5, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_test_lrpca15_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t5[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t5[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.6821713848641038,\n",
       " 0.001: 0.694242118712571,\n",
       " 0.01: 0.6940574180146195,\n",
       " 0.1: 0.6939552893933996,\n",
       " 1: 0.6939444246464611,\n",
       " 10: 0.6939335598995229}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t5 = {}\n",
    "aucs_svm_t5 = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_tr_lrpca15_ready, \n",
    "                                                                               Y_train_size_pca_cut5.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_pca_cut5, svm.predict(X_test_lrpca15_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_pca_cut5, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_test_lrpca15_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t5[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t5[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.5790214774317477,\n",
       " 0.001: 0.7104718776890249,\n",
       " 0.01: 0.7257933438214357,\n",
       " 0.1: 0.725836802809189,\n",
       " 1: 0.7258585323030656,\n",
       " 10: 0.7258737429487793}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5B. Same test data set and same features, only predicting on the actual positive instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only the postitve instances (not positive preds)\n",
    "X_tr_lrpca15_cut5 = X_tr_lrpca15_ysize.loc[X_tr_lrpca15_ysize['Y_max_new_fire_size_month']>0]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_train_size_pca_cut5b = X_tr_lrpca15_cut5['Y_max_new_fire_size_month']\n",
    "X_tr_lrpca15_ready2 = X_tr_lrpca15_cut5.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t5b = {}\n",
    "aucs_lr_t5b = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_tr_lrpca15_ready2, \n",
    "                                                                               Y_train_size_pca_cut5b.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_pca_cut5, lr.predict(X_test_lrpca15_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_pca_cut5, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_test_lrpca15_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t5b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t5b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.5672247634092218,\n",
       " 0.001: 0.5734422330813363,\n",
       " 0.01: 0.5682113418913954,\n",
       " 0.1: 0.5670424923113422,\n",
       " 1: 0.5669036190939101,\n",
       " 10: 0.5668804735576715}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t5b = {}\n",
    "aucs_svm_t5b = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_tr_lrpca15_ready2, \n",
    "                                                                               Y_train_size_pca_cut5b.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_pca_cut5, svm.predict(X_test_lrpca15_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_pca_cut5, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_test_lrpca15_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t5b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t5b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.46819369342001343,\n",
       " 0.001: 0.5308978442826185,\n",
       " 0.01: 0.5628213251398135,\n",
       " 0.1: 0.5676153443332494,\n",
       " 1: 0.568199769123276,\n",
       " 10: 0.568269205731992}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t5b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 6: Taking LR model-PCA, training on 2006-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 'LR_15PCA_1990_2005.pkl' model <br>\n",
    "use top 15 principle directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. slim down features\n",
    "X_ph1_lrpca15 = X_ph1_pca[X_train_pca.columns[0:15]]\n",
    "X_ph2_lrpca15 = X_ph2_pca[X_train_pca.columns[0:15]]\n",
    "X_ph3_lrpca15 = X_ph3_pca[X_train_pca.columns[0:15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. run model on train and test sets to get predictions\n",
    "y_preds_test6 = models['LR_15PCA_1990_2005.pkl'].predict(X_ph2_lrpca15)\n",
    "y_preds_6 = models['LR_15PCA_1990_2005.pkl'].predict(X_ph3_lrpca15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. store predictions and filter\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds6 = pd.DataFrame(y_preds_test6, columns=['preds'], index=X_ph2_lrpca15.index)\n",
    "X_ph2_lrpca15_preds = X_ph2_lrpca15.merge(preds6, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_ph2_lrpca15_ysize = X_ph2_lrpca15_preds.merge(Y_ph2_size_pca, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for the positive predicted instances\n",
    "X_ph2_lrpca15_cut = X_ph2_lrpca15_ysize.loc[X_ph2_lrpca15_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph2_size_pca_cut = X_ph2_lrpca15_cut['Y_max_new_fire_size_month']\n",
    "X_ph2_lrpca15_ready = X_ph2_lrpca15_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.5 need to score the test set and select only the positives to score on\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "test_set_preds_6 = pd.DataFrame(y_preds_6, columns=['preds'], index=X_ph3_lrpca15.index)\n",
    "X_ph3_lrpca15_preds = X_ph3_lrpca15.merge(test_set_preds_6, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_ph3_lrpca15_ysize  = X_ph3_lrpca15_preds.merge(Y_ph3_size_pca, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for only positive predictions \n",
    "X_ph3_lrpca15_cut = X_ph3_lrpca15_ysize.loc[X_ph3_lrpca15_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph3_size_pca_cut = X_ph3_lrpca15_cut['Y_max_new_fire_size_month']\n",
    "X_ph3_lrpca15_ready = X_ph3_lrpca15_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. run LR on all Cs and store all confusion matrices and AUCs\n",
    "aucs_lr_t6 = {}\n",
    "conf_mats_lr_t6 = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_ph2_lrpca15_ready, \n",
    "                                                                               Y_ph2_size_pca_cut.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_size_pca_cut, lr.predict(X_ph3_lrpca15_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_size_pca_cut, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_ph3_lrpca15_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t6[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t6[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.6531414468307674,\n",
       " 0.001: 0.6839123434997221,\n",
       " 0.01: 0.6855141218005295,\n",
       " 0.1: 0.6846233369291621,\n",
       " 1: 0.6844128992187245,\n",
       " 10: 0.684376123696512}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t6 = {}\n",
    "aucs_svm_t6 = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_ph2_lrpca15_ready, \n",
    "                                                                               Y_ph2_size_pca_cut.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_size_pca_cut, svm.predict(X_ph3_lrpca15_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_size_pca_cut, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_ph3_lrpca15_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t6[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t6[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.5007293811905462,\n",
       " 0.001: 0.6771374750743682,\n",
       " 0.01: 0.7241508940538066,\n",
       " 0.1: 0.7268579811055539,\n",
       " 1: 0.7270541172240201,\n",
       " 10: 0.7270786342388283}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6B. Same test data set and same features, only predicting on the actual positive instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only the postitve instances in the phase 2 set(not positive preds)\n",
    "X_ph2_lrpca15_cut6 = X_ph2_lrpca15_ysize.loc[X_ph2_lrpca15_ysize['Y_max_new_fire_size_month']>0]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph2_size_pca_cut6b = X_ph2_lrpca15_cut6['Y_max_new_fire_size_month']\n",
    "X_ph2_lrpca15_ready6 = X_ph2_lrpca15_cut6.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t6b = {}\n",
    "aucs_lr_t6b = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_ph2_lrpca15_ready6, \n",
    "                                                                               Y_ph2_size_pca_cut6b.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_size_pca_cut, lr.predict(X_ph3_lrpca15_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_size_pca_cut, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_ph3_lrpca15_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t6b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t6b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.5318417460996346,\n",
       " 0.001: 0.5467648829549882,\n",
       " 0.01: 0.5382054641891026,\n",
       " 0.1: 0.5338739951127953,\n",
       " 1: 0.5332038432934421,\n",
       " 10: 0.5331139448786507}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t6b = {}\n",
    "aucs_svm_t6b = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_ph2_lrpca15_ready6, \n",
    "                                                                               Y_ph2_size_pca_cut6b.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_size_pca_cut, svm.predict(X_ph3_lrpca15_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_size_pca_cut, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_ph3_lrpca15_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t6b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t6b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.44769682985499115,\n",
       " 0.001: 0.4835063650801867,\n",
       " 0.01: 0.5246634938882698,\n",
       " 0.1: 0.5360261086790109,\n",
       " 1: 0.5372765139029261,\n",
       " 10: 0.5374263445942449}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t6b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 7: Taking SVM model-PCA, training on 1990-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 'linSVC_25PCA_1990_2015.pkl model <br>\n",
    "use top 25 principle directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. slim down features\n",
    "X_tr_svmpca25 = X_train_pca[X_train_pca.columns[0:25]]\n",
    "X_test_svmpca25 = X_test_pca[X_test_pca.columns[0:25]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. run model on train and test sets to get predictions\n",
    "y_preds_test7 = models['linSVC_25PCA_1990_2015.pkl'].predict(X_tr_svmpca25)\n",
    "y_preds_7 = models['linSVC_25PCA_1990_2015.pkl'].predict(X_test_svmpca25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. store predictions and filter\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds7 = pd.DataFrame(y_preds_test7, columns=['preds'], index=X_tr_svmpca25.index)\n",
    "X_tr_svmpca25_preds = X_tr_svmpca25.merge(preds5, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_tr_svmpca25_ysize = X_tr_svmpca25_preds.merge(Y_train_size_pca, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for the positive predicted instances\n",
    "X_tr_svmpca25_cut = X_tr_svmpca25_ysize.loc[X_tr_svmpca25_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_train_size_pca_cut7 = X_tr_svmpca25_cut['Y_max_new_fire_size_month']\n",
    "X_tr_svmpca25_ready = X_tr_svmpca25_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.5 need to score the test set and select only the positives to score on\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "test_set_preds_7 = pd.DataFrame(y_preds_7, columns=['preds'], index=X_test_svmpca25.index)\n",
    "X_test_svmpca25_preds = X_test_svmpca25.merge(test_set_preds_7, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_test_svmpca25_ysize = X_test_svmpca25_preds.merge(Y_test_size_pca, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for only positive predictions \n",
    "X_test_svmpca25_cut = X_test_svmpca25_ysize.loc[X_test_svmpca25_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_test_size_pca_cut7 = X_test_svmpca25_cut['Y_max_new_fire_size_month']\n",
    "X_test_svmpca25_ready = X_test_svmpca25_cut.drop(columns=['preds','Y_max_new_fire_size_month'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t7 = {}\n",
    "aucs_lr_t7 = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_tr_svmpca25_ready, \n",
    "                                                                               Y_train_size_pca_cut7.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_pca_cut7, lr.predict(X_test_svmpca25_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_pca_cut7, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_test_svmpca25_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t7[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t7[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.6871411311201887,\n",
       " 0.001: 0.7010643806455323,\n",
       " 0.01: 0.7040676600886024,\n",
       " 0.1: 0.7044147824252537,\n",
       " 1: 0.7043956043956044,\n",
       " 10: 0.7044205358341485}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t7 = {}\n",
    "aucs_svm_t7 = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_tr_svmpca25_ready, \n",
    "                                                                               Y_train_size_pca_cut7.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_pca_cut7, svm.predict(X_test_svmpca25_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_pca_cut7, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_test_svmpca25_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t7[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t7[c] = cm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.586809351207257,\n",
       " 0.001: 0.714843794948507,\n",
       " 0.01: 0.7369330494984945,\n",
       " 0.1: 0.737870855148342,\n",
       " 1: 0.7379801699173426,\n",
       " 10: 0.7379801699173427}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7B. Same test data set and same features, only predicting on the actual positive instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only the postitve instances (not positive preds)\n",
    "X_tr_svmpca25_cut7 = X_tr_svmpca25_ysize.loc[X_tr_lrpca15_ysize['Y_max_new_fire_size_month']>0]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_train_size_pca_cut7b = X_tr_svmpca25_cut7['Y_max_new_fire_size_month']\n",
    "X_tr_svmpca25_ready2 = X_tr_svmpca25_cut7.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t7b = {}\n",
    "aucs_lr_t7b = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_tr_svmpca25_ready2, \n",
    "                                                                               Y_train_size_pca_cut7b.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_pca_cut7, lr.predict(X_test_svmpca25_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_pca_cut7, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_test_svmpca25_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t7b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t7b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.5623877894456862,\n",
       " 0.001: 0.5574057687094316,\n",
       " 0.01: 0.5396592379656577,\n",
       " 0.1: 0.5315340429558626,\n",
       " 1: 0.5299560620553136,\n",
       " 10: 0.5298460893183224}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t7b = {}\n",
    "aucs_svm_t7b = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_tr_svmpca25_ready2, \n",
    "                                                                               Y_train_size_pca_cut7b.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_test_size_pca_cut7, svm.predict(X_test_svmpca25_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_test_size_pca_cut7, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_test_svmpca25_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t7b[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t7b[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.4368321713733293,\n",
       " 0.001: 0.4986087170017851,\n",
       " 0.01: 0.5325698326879894,\n",
       " 0.1: 0.5380249919438577,\n",
       " 1: 0.538605545694951,\n",
       " 10: 0.5386745983437595}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 8: Taking SVM model-PCA, training on 2006-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 'linSVC_15PCA_1990_2005.pkl' model <br>\n",
    "use top 15 principle directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. slim down features\n",
    "X_ph1_svmpca15 = X_ph1_pca[X_train_pca.columns[0:15]]\n",
    "X_ph2_svmpca15 = X_ph2_pca[X_train_pca.columns[0:15]]\n",
    "X_ph3_svmpca15 = X_ph3_pca[X_train_pca.columns[0:15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. run model on train and test sets to get predictions\n",
    "y_preds_test8 = models['linSVC_15PCA_1990_2005.pkl'].predict(X_ph2_svmpca15)\n",
    "y_preds_8 = models['linSVC_15PCA_1990_2005.pkl'].predict(X_ph3_svmpca15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. store predictions and filter\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "preds8 = pd.DataFrame(y_preds_test8, columns=['preds'], index=X_ph2_svmpca15.index)\n",
    "X_ph2_svmpca15_preds = X_ph2_svmpca15.merge(preds8, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_ph2_svmpca15_ysize = X_ph2_svmpca15_preds.merge(Y_ph2_size_pca, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for the positive predicted instances\n",
    "X_ph2_svmpca15_cut = X_ph2_svmpca15_ysize.loc[X_ph2_svmpca15_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph2_size_pca_cut8 = X_ph2_svmpca15_cut['Y_max_new_fire_size_month']\n",
    "X_ph2_svmpca15_ready = X_ph2_svmpca15_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.5 need to score the test set and select only the positives to score on\n",
    "\n",
    "#append the these predictions to the dataframe\n",
    "test_set_preds_8 = pd.DataFrame(y_preds_8, columns=['preds'], index=X_ph3_svmpca15.index)\n",
    "X_ph3_svmpca15_preds = X_ph3_svmpca15.merge(test_set_preds_8, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#rejoin with the y-size column\n",
    "X_ph3_svmpca15_ysize  = X_ph3_svmpca15_preds.merge(Y_ph3_size_pca, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#filter for only positive predictions \n",
    "X_ph3_svmpca15_cut = X_ph3_svmpca15_ysize.loc[X_ph3_svmpca15_ysize['preds']==1]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph3_size_pca_cut8 = X_ph3_svmpca15_cut['Y_max_new_fire_size_month']\n",
    "X_ph3_svmpca15_ready = X_ph3_svmpca15_cut.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. run LR on all Cs and store all confusion matrices and AUCs\n",
    "aucs_lr_t8 = {}\n",
    "conf_mats_lr_t8 = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_ph2_svmpca15_ready, \n",
    "                                                                               Y_ph2_size_pca_cut8.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_size_pca_cut8, lr.predict(X_ph3_svmpca15_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_size_pca_cut8, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_ph3_svmpca15_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t8[c] = np.mean(aucs_sub)\n",
    "    conf_mats_lr_t8[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0.0001: 0.6739553214924962,\n",
       "  0.001: 0.6976989536004407,\n",
       "  0.01: 0.698635205837808,\n",
       "  0.1: 0.6980380008261049,\n",
       "  1: 0.6979777640093625,\n",
       "  10: 0.6979708798017349},\n",
       " {0.0001: array([[0.46786042, 0.11707989, 0.19972452, 0.21533517],\n",
       "         [0.23463687, 0.19832402, 0.30726257, 0.25977654],\n",
       "         [0.18181818, 0.11038961, 0.35064935, 0.35714286],\n",
       "         [0.15277778, 0.0787037 , 0.26388889, 0.50462963]]),\n",
       "  0.001: array([[0.52892562, 0.10606061, 0.15794307, 0.20707071],\n",
       "         [0.31005587, 0.15363128, 0.27653631, 0.25977654],\n",
       "         [0.2012987 , 0.12337662, 0.30519481, 0.37012987],\n",
       "         [0.16666667, 0.05555556, 0.25462963, 0.52314815]]),\n",
       "  0.01: array([[0.55004591, 0.10284665, 0.14416896, 0.20293848],\n",
       "         [0.31005587, 0.1452514 , 0.29608939, 0.24860335],\n",
       "         [0.18181818, 0.12337662, 0.31818182, 0.37662338],\n",
       "         [0.18981481, 0.0462963 , 0.26388889, 0.5       ]]),\n",
       "  0.1: array([[0.55004591, 0.10192837, 0.14600551, 0.2020202 ],\n",
       "         [0.31005587, 0.1452514 , 0.29888268, 0.24581006],\n",
       "         [0.18831169, 0.11688312, 0.31818182, 0.37662338],\n",
       "         [0.19444444, 0.0462963 , 0.25925926, 0.5       ]]),\n",
       "  1: array([[0.55050505, 0.10238751, 0.14646465, 0.20064279],\n",
       "         [0.31005587, 0.1452514 , 0.29888268, 0.24581006],\n",
       "         [0.18831169, 0.11688312, 0.31818182, 0.37662338],\n",
       "         [0.19444444, 0.0462963 , 0.25925926, 0.5       ]]),\n",
       "  10: array([[0.55004591, 0.10238751, 0.14646465, 0.20110193],\n",
       "         [0.31005587, 0.1452514 , 0.29888268, 0.24581006],\n",
       "         [0.18831169, 0.11688312, 0.31818182, 0.37662338],\n",
       "         [0.19444444, 0.0462963 , 0.25925926, 0.5       ]])})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t8, conf_mats_lr_t8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t8 = {}\n",
    "aucs_svm_t8 = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_ph2_svmpca15_ready, \n",
    "                                                                               Y_ph2_size_pca_cut8.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_size_pca_cut8, svm.predict(X_ph3_svmpca15_ready), normalize='true')\n",
    "    \n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_size_pca_cut8, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_ph3_svmpca15_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t8[c] = np.mean(aucs_sub)\n",
    "    conf_mats_svm_t8[c] = cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0.0001: 0.5439900867410161,\n",
       "  0.001: 0.7025712515489467,\n",
       "  0.01: 0.7318893707834229,\n",
       "  0.1: 0.7333815227867272,\n",
       "  1: 0.7335587911331406,\n",
       "  10: 0.7335846069117444},\n",
       " {0.0001: array([[0.89348026, 0.03627181, 0.02433425, 0.04591368],\n",
       "         [0.88268156, 0.02793296, 0.03351955, 0.05586592],\n",
       "         [0.87012987, 0.03896104, 0.04545455, 0.04545455],\n",
       "         [0.85185185, 0.02777778, 0.02314815, 0.09722222]]),\n",
       "  0.001: array([[0.9164371 , 0.02571166, 0.0174472 , 0.04040404],\n",
       "         [0.84078212, 0.01396648, 0.06145251, 0.08379888],\n",
       "         [0.74675325, 0.04545455, 0.07142857, 0.13636364],\n",
       "         [0.69907407, 0.02777778, 0.06018519, 0.21296296]]),\n",
       "  0.01: array([[0.90955005, 0.02112029, 0.02571166, 0.043618  ],\n",
       "         [0.73463687, 0.03072626, 0.10335196, 0.13128492],\n",
       "         [0.61038961, 0.06493506, 0.12987013, 0.19480519],\n",
       "         [0.62037037, 0.03703704, 0.08333333, 0.25925926]]),\n",
       "  0.1: array([[0.90312213, 0.02157943, 0.02800735, 0.04729109],\n",
       "         [0.70949721, 0.03910615, 0.10614525, 0.1452514 ],\n",
       "         [0.59090909, 0.06493506, 0.13636364, 0.20779221],\n",
       "         [0.61574074, 0.03703704, 0.09259259, 0.25462963]]),\n",
       "  1: array([[0.90266299, 0.02157943, 0.02800735, 0.04775023],\n",
       "         [0.70670391, 0.04189944, 0.10614525, 0.1452514 ],\n",
       "         [0.58441558, 0.06493506, 0.13636364, 0.21428571],\n",
       "         [0.61574074, 0.03703704, 0.09259259, 0.25462963]]),\n",
       "  10: array([[0.90220386, 0.02157943, 0.02800735, 0.04820937],\n",
       "         [0.70670391, 0.04189944, 0.10614525, 0.1452514 ],\n",
       "         [0.58441558, 0.06493506, 0.13636364, 0.21428571],\n",
       "         [0.61574074, 0.03703704, 0.09259259, 0.25462963]])})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t8, conf_mats_svm_t8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8B. Same test data set and same features, only predicting on the actual positive instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only the postitve instances in the phase 2 set(not positive preds)\n",
    "X_ph2_svmpca15_cut8 = X_ph2_svmpca15_ysize.loc[X_ph2_svmpca15_ysize['Y_max_new_fire_size_month']>0]\n",
    "\n",
    "\n",
    "#drop preds and class_size columns, while saving class size as a new training y\n",
    "Y_ph2_size_pca_cut8b = X_ph2_svmpca15_cut8['Y_max_new_fire_size_month']\n",
    "X_ph2_svmpca15_ready8 = X_ph2_svmpca15_cut8.drop(columns=['preds','Y_max_new_fire_size_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LR on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_lr_t8b = {}\n",
    "aucs_lr_t8b = {}\n",
    "for c in cs:\n",
    "    lr = LogisticRegression(C=c, max_iter=1500, class_weight = 'balanced').fit(X_ph2_svmpca15_ready8, \n",
    "                                                                               Y_ph2_size_pca_cut8b.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_size_pca_cut8, lr.predict(X_ph3_svmpca15_ready), normalize='true')\n",
    "    conf_mats_lr_t8b[c] = cm\n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_size_pca_cut8, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], lr.predict_proba(X_ph3_svmpca15_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_lr_t8b[c] = np.mean(aucs_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.5419930723346421,\n",
       " 0.001: 0.5662139459377832,\n",
       " 0.01: 0.5612565123829658,\n",
       " 0.1: 0.5577053760193296,\n",
       " 1: 0.557157958320749,\n",
       " 10: 0.5570635759589248}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_lr_t8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run SVM on all Cs and store all confusion matrices and AUCs\n",
    "conf_mats_svm_t8b = {}\n",
    "aucs_svm_t8b = {}\n",
    "for c in cs:\n",
    "    svm = LinearSVC(C=c, class_weight = 'balanced', dual=False).fit(X_ph2_svmpca15_ready8, \n",
    "                                                                               Y_ph2_size_pca_cut8b.to_numpy().ravel())\n",
    "\n",
    "    cm = confusion_matrix(Y_ph3_size_pca_cut8, svm.predict(X_ph3_svmpca15_ready), normalize='true')\n",
    "    conf_mats_svm_t8b[c] = cm\n",
    "    #store aucs -- will aggregate after taking an auc for each class -- simple mean\n",
    "    y = label_binarize(Y_ph3_size_pca_cut8, classes=[0,1,2,3])\n",
    "    n_classes = y.shape[1]\n",
    "    aucs_sub = np.zeros(n_classes)\n",
    "    for n in range(n_classes-1):\n",
    "        fpr, tpr, thresholds = roc_curve(y[:,n], svm.decision_function(X_ph3_svmpca15_ready)[:,n])\n",
    "        aucs_sub = auc(fpr,tpr)\n",
    "    \n",
    "    aucs_svm_t8b[c] = np.mean(aucs_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0001: 0.44227810706735127,\n",
       " 0.001: 0.49871639987919053,\n",
       " 0.01: 0.5454710623678647,\n",
       " 0.1: 0.5577808819087888,\n",
       " 1: 0.5591588643914225,\n",
       " 10: 0.5593311122017517}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_svm_t8b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
