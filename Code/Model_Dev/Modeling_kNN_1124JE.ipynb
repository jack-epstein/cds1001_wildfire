{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd  # provides interface for interacting with tabular data\n",
    "import geopandas as gpd  # combines the capabilities of pandas and shapely for geospatial operations\n",
    "import rtree  # supports geospatial join\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "from shapely.ops import nearest_points\n",
    "from datetime import datetime as dt, date\n",
    "sys.path.append('/Users/jackepstein/Documents/GitHub/wildfires-1001/code/functions/')\n",
    "from modeling_functions import *\n",
    "data_dir = '/Users/jackepstein/Documents/GitHub/wildfires-1001/data'\n",
    "code_dir = '/Users/jackepstein/Documents/GitHub/wildfires-1001/code'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull in main data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in the target data frame and weather dictionary \n",
    "#make sure to change the pkl file name if needed\n",
    "target_dict = {}\n",
    "target_df = gpd.GeoDataFrame()\n",
    "for i in np.arange(1, 3):\n",
    "    target_dict[i] = pd.read_pickle(os.path.join(data_dir, f'clean_data/target_df_final_1123_newtargets_{i}.pkl')) \n",
    "    target_df = target_df.append(target_dict[i])\n",
    "\n",
    "\n",
    "weather_dict_path = os.path.join(data_dir, 'clean_data/ERA_weather-data/ERA_rename_dictionary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the naming dictionary\n",
    "with open(weather_dict_path, 'rb') as handle:\n",
    "    rename_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the columns based on this dictionary\n",
    "target_df.rename(columns = rename_dict, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists of columns to drop and what our targets are\n",
    "non_mod_cols = ['GRID_ID','month_id','MONTH','COUNTYFP','NAME','GRID_AREA','COUNTY_ARE','COUNTY_AREA',\n",
    "                'geometry', 'adj_fire_count','adj_fire_bcount', 'Fire_area','Index','index']\n",
    "bad_features = ['hist_p_time_1m', 'total_fire_days', 'hist_p_time_1y','month_id_old']\n",
    "Y_cols = ['Y_bin', 'Y_fire_count', 'Y_fire_area_prop', 'Y_fire_class_size','Y_bin_new_fire_month',\n",
    "          'Y_max_new_fire_size_month','Y_count_new_fires_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert floats from 64 to 32 for model\n",
    "for col in target_df.columns:\n",
    "    if target_df[col].dtypes == 'float64':\n",
    "        target_df[col] = target_df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate training data set\n",
    "#pre 2016\n",
    "train_data = target_df[target_df['YEAR']<=2016]\n",
    "X_train = train_data.drop('YEAR', axis = 1)\n",
    "#drop columns not used for modeling\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_train.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "#set up target variable\n",
    "Y_train_reg = train_data[['Y_fire_area_prop']]\n",
    "Y_train_cl = train_data[['Y_bin_new_fire_month']]\n",
    "Y_train_cl_size = train_data[['Y_max_new_fire_size_month']]\n",
    "Y_train_cl_arr = Y_train_cl.to_numpy().ravel()\n",
    "\n",
    "#generate testing data set - same logic as above\n",
    "test_data = target_df[target_df['YEAR']>2016]\n",
    "X_test = test_data.drop('YEAR', axis = 1)\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_test.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "Y_test_reg = test_data[['Y_fire_area_prop']]\n",
    "Y_test_cl = test_data[['Y_bin_new_fire_month']]\n",
    "Y_test_cl_size = test_data[['Y_max_new_fire_size_month']]\n",
    "Y_test_cl_arr = Y_test_cl.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for any null values\n",
    "\n",
    "#null vals array\n",
    "null = np.zeros(len(X_train.columns))\n",
    "\n",
    "for i in range(len(X_train.columns)):\n",
    "    null[i] = X_train.loc[X_train[X_train.columns[i]].isna()].shape[0]\n",
    "    \n",
    "np.sum(null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale test and training sets\n",
    "X_train_scaled = pd.DataFrame(scale(X_train), columns = X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scale(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in top features from decision trees and random forest and send to list\n",
    "dt_feat_df = pd.read_csv(os.path.join(data_dir, 'feature_importances/featureimportance_DT.csv'), index_col=0)\n",
    "dt_feat_df = dt_feat_df.rename(columns={\"feature score\": \"feature_importance\", \"feature\": \"col\"})\n",
    "feat_list_dt = dt_feat_df['col'].to_list()[:25]\n",
    "\n",
    "rf_feat_df = pd.read_csv(os.path.join(data_dir, 'feature_importances/featureimportance_RF_JE.csv'), index_col=0)\n",
    "rf_feat_df = rf_feat_df.rename(columns={\"feature score\": \"feature_importance\", \"feature\": \"col\"})\n",
    "feat_list_rf = rf_feat_df['col'].to_list()[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slim down training and test data to only these features\n",
    "X_train_scaled_dt = X_train_scaled[feat_list_dt]\n",
    "X_train_scaled_rf = X_train_scaled[feat_list_rf]\n",
    "\n",
    "X_test_scaled_dt = X_test_scaled[feat_list_dt]\n",
    "X_test_scaled_rf = X_test_scaled[feat_list_rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use function to select features -- will be slightly different\n",
    "feat_list_dt_2, corr_dt = select_features_corr_imp(X_train_scaled, dt_feat_df, 0.75, 25)\n",
    "feat_list_rf_2, corr_fr = select_features_corr_imp(X_train_scaled, dt_feat_df, 0.75, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use new features\n",
    "X_train_scaled_dt2 = X_train_scaled[feat_list_dt_2]\n",
    "X_train_scaled_rf2 = X_train_scaled[feat_list_rf_2]\n",
    "\n",
    "X_test_scaled_dt2 = X_test_scaled[feat_list_dt_2]\n",
    "X_test_scaled_rf2 = X_test_scaled[feat_list_rf_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test KNN - on DT features list, k=1,..,20\n",
    "- Using Euclidean distance (Minkowski, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check different k\n",
    "recall_dt = []\n",
    "auc_dt = []\n",
    "for k in range(1,21):\n",
    "    #train the model\n",
    "    knn_dt = KNeighborsClassifier(n_neighbors=k).fit(X_train_scaled_dt, Y_train_cl_arr)\n",
    "    \n",
    "    #make predictions\n",
    "    preds = knn_dt.predict(X_test_scaled_dt)\n",
    "    preds_prob = knn_dt.predict_proba(X_test_scaled_dt)\n",
    "    \n",
    "    #generate report and store auc and recall\n",
    "    recall = classification_report(Y_test_cl_arr, preds, output_dict=True)['1']['recall']\n",
    "    auc = roc_auc_score(Y_test_cl_arr, preds_prob[:,1])\n",
    "    \n",
    "    #add these to lists\n",
    "    recall_dt.append(recall)\n",
    "    auc_dt.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-trying with new lists - add ev calc\n",
    "recall_dt2 = []\n",
    "auc_dt2 = []\n",
    "ev_dt2 = []\n",
    "for k in range(1,21):\n",
    "    #train the model\n",
    "    knn_dt = KNeighborsClassifier(n_neighbors=k).fit(X_train_scaled_dt2, Y_train_cl_arr)\n",
    "    \n",
    "    #make predictions\n",
    "    preds = knn_dt.predict(X_test_scaled_dt2)\n",
    "    preds_prob = knn_dt.predict_proba(X_test_scaled_dt2)\n",
    "    \n",
    "    #generate report and store auc and recall\n",
    "    recall = classification_report(Y_test_cl_arr, preds, output_dict=True)['1']['recall']\n",
    "    auc = roc_auc_score(Y_test_cl_arr, preds_prob[:,1])\n",
    "    ev = EV_binary(Y_test_cl_arr, preds, V_tp = 0, V_tn = 0, C_fp = -0.1875, C_fn = -0.75)\n",
    "    \n",
    "    #add these to lists\n",
    "    recall_dt2.append(recall)\n",
    "    auc_dt2.append(auc)\n",
    "    ev_dt2.append(ev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test KNN - on RF features list, k=1,..,20\n",
    "- Using Euclidean distance (Minkowski, p=2)"
   ]
  },
   ],
   "source": [
    "#check different k\n",
    "recall_rf = []\n",
    "auc_rf = []\n",
    "for k in range(1,21):\n",
    "    #train the model\n",
    "    knn_rf = KNeighborsClassifier(n_neighbors=k).fit(X_train_scaled_rf, Y_train_cl_arr)\n",
    "    \n",
    "    #make predictions\n",
    "    preds = knn_rf.predict(X_test_scaled_rf)\n",
    "    preds_prob = knn_rf.predict_proba(X_test_scaled_rf)\n",
    "    \n",
    "    #generate report and store auc and recall\n",
    "    recall = classification_report(Y_test_cl_arr, preds, output_dict=True)['1']['recall']\n",
    "    auc = roc_auc_score(Y_test_cl_arr, preds_prob[:,1])\n",
    "    \n",
    "    #add these to lists\n",
    "    recall_rf.append(recall)\n",
    "    auc_rf.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check different k\n",
    "#new lists\n",
    "recall_rf2 = []\n",
    "auc_rf2 = []\n",
    "for k in range(1,21):\n",
    "    #train the model\n",
    "    knn_rf = KNeighborsClassifier(n_neighbors=k).fit(X_train_scaled_rf2, Y_train_cl_arr)\n",
    "    \n",
    "    #make predictions\n",
    "    preds = knn_rf.predict(X_test_scaled_rf2)\n",
    "    preds_prob = knn_rf.predict_proba(X_test_scaled_rf2)\n",
    "    \n",
    "    #generate report and store auc and recall\n",
    "    recall = classification_report(Y_test_cl_arr, preds, output_dict=True)['1']['recall']\n",
    "    auc = roc_auc_score(Y_test_cl_arr, preds_prob[:,1])\n",
    "    \n",
    "    #add these to lists\n",
    "    recall_rf2.append(recall)\n",
    "    auc_rf2.append(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear that as we include more neighbords, we guess fewer fires. This is because we have too many negative instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check one test on all features, k=1 and 20 \n",
    "- Using euclidean distance\n",
    "- Want to see if we get same extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit on all data\n",
    "knn_full_1 = KNeighborsClassifier(n_neighbors=1).fit(X_train_scaled, Y_train_cl_arr)\n",
    "\n",
    "#get predictions\n",
    "full_preds_1 = knn_full_1.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3857  319]\n",
      " [ 397  179]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92      4176\n",
      "           1       0.36      0.31      0.33       576\n",
      "\n",
      "    accuracy                           0.85      4752\n",
      "   macro avg       0.63      0.62      0.62      4752\n",
      "weighted avg       0.84      0.85      0.84      4752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print report\n",
    "print(confusion_matrix(Y_test_cl_arr, full_preds_1))\n",
    "print(classification_report(Y_test_cl_arr, full_preds_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit on all data\n",
    "knn_full_20 = KNeighborsClassifier(n_neighbors=20).fit(X_train_scaled, Y_train_cl_arr)\n",
    "\n",
    "#get predictions\n",
    "full_preds_20 = knn_full_20.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4145   31]\n",
      " [ 516   60]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94      4176\n",
      "           1       0.66      0.10      0.18       576\n",
      "\n",
      "    accuracy                           0.88      4752\n",
      "   macro avg       0.77      0.55      0.56      4752\n",
      "weighted avg       0.86      0.88      0.85      4752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print report\n",
    "print(confusion_matrix(Y_test_cl_arr, full_preds_20))\n",
    "print(classification_report(Y_test_cl_arr, full_preds_20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing mahalanobis distance\n",
    "- CAN'T TRY ON FULL SET\n",
    "- Shold use feature correlation function to pull in a sublist and test on this"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#fit on all data\n",
    "knn_full_mal = KNeighborsClassifier(n_neighbors=1, metric='mahalanobis',\n",
    "                                    metric_params={'V': np.cov(X_train_scaled_dt2)}).fit(X_train_scaled_dt2, Y_train_cl_arr)\n",
    "\n",
    "#get predictions\n",
    "full_preds_mal = knn_full_mal.predict(X_test_scaled_dt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#print report\n",
    "print(confusion_matrix(Y_test_cl_arr, full_preds_mal))\n",
    "print(classification_report(Y_test_cl_arr, full_preds_mal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try doing only on months in fire season\n",
    "#pick may - november\n",
    "fire_season = target_df.loc[target_df['MONTH']>=5].loc[target_df['MONTH']<=11]\n",
    "\n",
    "#generate training data set - on fire season\n",
    "#pre 2016\n",
    "train_data_fs = fire_season[fire_season['YEAR']<=2016]\n",
    "X_train_fs = train_data_fs.drop('YEAR', axis = 1)\n",
    "#drop columns not used for modeling\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_train_fs.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "#set up target variable\n",
    "Y_train_bin_fs = train_data_fs[['Y_bin_new_fire_month']]\n",
    "\n",
    "#generate testing data set - same logic as above\n",
    "test_data_fs = fire_season[fire_season['YEAR']>2016]\n",
    "X_test_fs = test_data_fs.drop('YEAR', axis = 1)\n",
    "for y in Y_cols + non_mod_cols + bad_features:\n",
    "    try:\n",
    "        X_test_fs.drop(y, inplace = True, axis =1)\n",
    "    except:\n",
    "        pass\n",
    "Y_test_bin_fs = test_data_fs[['Y_bin_new_fire_month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data#scale test and training sets\n",
    "X_train_scaled_fs = pd.DataFrame(scale(X_train_fs), columns = X_train_fs.columns)\n",
    "X_test_scaled_fs = pd.DataFrame(scale(X_test_fs), columns = X_test_fs.columns)\n",
    "\n",
    "#feature selection\n",
    "X_train_scaled_fs = X_train_scaled_fs[feat_list_rf_2]\n",
    "X_test_scaled_fs = X_test_scaled_fs[feat_list_rf_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run another kNN loop on the downsampled size\n",
    "#re-trying with new lists - add ev calc\n",
    "recall_fs = {}\n",
    "auc_fs = {}\n",
    "ev_fs = {}\n",
    "for k in range(1,21):\n",
    "    #train the model\n",
    "    knn_fs = KNeighborsClassifier(n_neighbors=k).fit(X_train_scaled_fs, Y_train_bin_fs.to_numpy().ravel())\n",
    "    \n",
    "    #make predictions\n",
    "    preds = knn_fs.predict(X_test_scaled_fs)\n",
    "    preds_prob = knn_fs.predict_proba(X_test_scaled_fs)\n",
    "    \n",
    "    #generate report and store auc and recall\n",
    "    recall = classification_report(Y_test_bin_fs.to_numpy().ravel(), preds, output_dict=True)['1']['recall']\n",
    "    auc = roc_auc_score(Y_test_bin_fs.to_numpy().ravel(), preds_prob[:,1])\n",
    "    ev = EV_binary(Y_test_bin_fs.to_numpy().ravel(), preds, V_tp = 0, V_tn = 0, C_fp = -0.1875, C_fn = -0.75)\n",
    "    \n",
    "    #add these to lists\n",
    "    recall_fs[k]=recall\n",
    "    auc_fs[k] = auc\n",
    "    ev_fs[k]= ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11444805194805194"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(ev_fs.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
